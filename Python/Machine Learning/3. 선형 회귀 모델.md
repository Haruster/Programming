# 선형 회귀는 지도학습의 대표적인 예시이다.

## 회귀란 평균으로 돌아가는 것을 말한다.


- y = ax + b == H(x) = Wx + b

-	a = 기울기

-	b = 절편

-	y = H(x)

-	a = W

-	b = b

### H(x)는 Hypothesis(가설)을 나타낸다. -> 회귀라는 것은 데이터를 잘 나타내는 선을 찾아가는 것이다. 즉 이것을 하나의 가설이라고 할 수 있다.(즉, 그래프의 선은 데이터들을 잘 찾아낸다는 가설을 세워서 진행해 나가는 것이라고도 할 수 있다.) 

- 추가로 그래프 상에 나타나 있는 별은 모두 데이터이다.

- 데이터들은 모두 차이점을 가지고 있기 때문에 정확하게 이를 찾아야 하는데 이를 위해서는 데이터와 가설 즉, 선과의 차이점을 보는 것이 중요하다.

- 데이터들은 y라고 한다. (y = 데이터)

- 즉, 데이터들을 정확하게 찾기 위한 방법을 식으로 나타내자면 H(x) - y이며 이 값을 최소화 시켜야 한다.
- 하지만 무작정 H(x) - y를 구하게 된다면 각 데이터마다 음수 양수가 각각 나오므로 이는 문제를 발생시킬 수 있기 때문에 이를 방지하기 위해서 
- H(x) - y의 식에 제곱을 취한다.

- 그리고 이것을 식으로 나타내자면 MSE(3. 선형회귀 제곱식.png)에 관한 식이 나온다.


## 1/m를 하는 이유는 MSE가 평균제곱오차(2차함수)이기 때문이다. (차이들의 평균을 내기 위해서이다.)

## 비용함수 ( cost(w) ) 란 : 가설이 잘 맞는지 안 맞는지 검사하기 위해서 사용하는 함수이다.

## 선형회귀 분석이란? : 회귀식에서 MSE를 통해 가장 적합한 w와 b값을 찾는 것.

## 이런 선형회귀 분석에 가장 대표적인 방법이 바로 경사 하강법이다.

## 경사하강법 - 비용함수 값이 최소가 되게 하는 w와 b값을 찾는 것(값을 하강시킴)

### 찾는 법 : 임의의 w값을 선정한다(초깃값) -> 경사하강법에 맞게 조금씩 값을 하강시킨다.(구제적으로 보자면 비용함수를 w에 대해 편미분을 해준 것의 학습률이라고 불리는 파라미터를 곱한 것을 초기값 w에서 빼주는 것)

- 이를 수식으로 나타내면 3. 경사하강법 최적의 w값 찾기.png와 같다. (b값을 찾는 것도 이와 수식이 같다.(최적의 b값 찾기.png 참조))(w대신 b사용)

- a(alpha) = 파라미터

- a(alpha(Learning Rate)) 값에 따라서 값이 달라질 수 있는데 
- a(alpha) 값이 너무 작으면 움직이는 값이 너무 작다보니 찾는데 시간이 너무 오래 걸리게 되고 너무 크면 너무 크게 움직이다보니 최적의 값을 지나치게 되니 문제가 발생할 수 있다.(즉, 적절한 alpha값을 잘 설정하는게 중요하다. )


## 이렇게 w에 대해 편미분을 하게 된다면 현재 w위치에서의 접선의 기울기와 같게 된다.

- 이 과정을 거치면 w의 위치가 살짝 하강하게 되는데 이를 계속 최소값이 나올 때까지 반복해주면 된다. (접선의 기울기 값이 0이 될때가 최적의 w값이다.)

### 경사하강법의 단점
- 복잡한 그래프의 경우에서는 정확한 최소값을 찾아내기 어렵다는 단점이 있다.







